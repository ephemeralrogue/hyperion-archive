volumes:
  ollama_vol:
    driver: local
  anythingllm_vol:
    driver: local

networks:
  panic:
    driver: bridge

services:

  ollama:
    image: ollama/ollama:latest
    environment:
      - OLLAMA_DIR=${OLLAMA_DIR}
    volumes:
      - ollama_vol:${OLLAMA_DIR}  # Persistent storage for models and settings
    container_name: ollama
    restart: no
    ports:
      - 11434:11434  # Expose Ollama's API
    networks:
      - panic
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    # healthcheck:
      # test:
        # [
          # "CMD-SHELL",
          # "curl -s -I http://localhost:11434 | grep -q 'HTTP/1.1 302 Found'",
        # ]
      # interval: 10s
      # timeout: 10s
      # retries: 120

  archive:
    depends_on:
      - ollama
        # condition: service_healthy
    container_name: LOADERBOT
    image: mintplexlabs/anythingllm:latest
    cap_add:
      - SYS_ADMIN
    volumes:
      - ./archive:/app/server/storage
      - ./.env:/app/server/.env:rw
      - ./collector/hotdir/:/app/collector/hotdir
      - ./collector/outputs/:/app/collector/outputs
    ports:
      - 3001:3001
    environment:
      - STORAGE_LOCATION=${STORAGE_LOCATION}
      - OLLAMA_SERVICE_URL=${OLLAMA_BASE_PATH}
      - LLM_SERVICE_URL=${LLM_SERVICE_URL}
      - LLM_PROVIDER=${LLM_PROVIDER}
    env_file:
      - .env
    networks:
      - panic